"""
GPU ê°ì§€ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""
import torch

print("=" * 60)
print("ğŸ” PyTorch GPU ê°ì§€ í™•ì¸")
print("=" * 60)

print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")

# ROCm í™•ì¸
if hasattr(torch.version, 'hip') and torch.version.hip:
    print(f"ROCm ë²„ì „: {torch.version.hip}")
    print("âœ… ROCm (AMD GPU) ì§€ì› í™œì„±í™”ë¨")
else:
    print("ROCm: ì‚¬ìš© ë¶ˆê°€")

# CUDA í™•ì¸
if hasattr(torch.version, 'cuda') and torch.version.cuda:
    print(f"CUDA ë²„ì „: {torch.version.cuda}")
    print("âœ… CUDA (NVIDIA GPU) ì§€ì› í™œì„±í™”ë¨")

print("-" * 60)

if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"âœ… GPU ê°ì§€ë¨: {device_count}ê°œ")
    
    for i in range(device_count):
        print(f"\nGPU {i}:")
        print(f"  ì´ë¦„: {torch.cuda.get_device_name(i)}")
        props = torch.cuda.get_device_properties(i)
        print(f"  ì´ ë©”ëª¨ë¦¬: {props.total_memory / (1024**3):.2f} GB")
        print(f"  ì»´í“¨íŒ… ëŠ¥ë ¥: {props.major}.{props.minor}")
    
    device = torch.device('cuda')
    print(f"\nâœ… ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
    print("ğŸš€ GPUë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
else:
    print("âš ï¸ GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
    device = torch.device('cpu')
    print(f"ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")

print("=" * 60)

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
print("\nê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
try:
    x = torch.randn(3, 3).to(device)
    y = torch.randn(3, 3).to(device)
    z = torch.matmul(x, y)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ë””ë°”ì´ìŠ¤: {device}")
    print(f"   ê²°ê³¼ í…ì„œ ìœ„ì¹˜: {z.device}")
except Exception as e:
    print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

print("=" * 60)
