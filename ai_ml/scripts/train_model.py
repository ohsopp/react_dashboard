"""
AI ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (PyTorch)
- ì˜¨ë„ì™€ ì§„ë™ ì„¼ì„œì˜ ìƒê´€ê´€ê³„ í•™ìŠµ
- LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸
"""
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from influxdb_client import InfluxDBClient
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import os
import pickle
import json
import time
try:
    import psutil
except ImportError:
    psutil = None

# InfluxDB ì„¤ì •
INFLUXDB_URL = 'http://localhost:8090'
INFLUXDB_TOKEN = 'my-super-secret-auth-token'
INFLUXDB_ORG = 'my-org'
INFLUXDB_BUCKET_TEMP = 'temperature_augmented'
INFLUXDB_BUCKET_VIB = 'vibration_augmented'

# ëª¨ë¸ ì„¤ì •
MODEL_DIR = os.path.join(os.path.dirname(__file__), '..', 'models')
SEQUENCE_LENGTH = 30  # 30ê°œ ì‹œì ìœ¼ë¡œ ë‹¤ìŒ ê°’ ì˜ˆì¸¡ (60 -> 30ìœ¼ë¡œ ì¤„ì—¬ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•)
BATCH_SIZE = 256  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ (128 -> 256)
EPOCHS = 30  # ì—í¬í¬ ìˆ˜ ê°ì†Œ (50 -> 30)
LEARNING_RATE = 0.002  # í•™ìŠµë¥  ì¦ê°€ë¡œ ë¹ ë¥¸ ìˆ˜ë ´ (0.001 -> 0.002)

# ì§„í–‰ë¥  íŒŒì¼ ê²½ë¡œ
PROGRESS_FILE = os.path.join(os.path.dirname(__file__), '..', 'data', 'train_progress.json')

def save_progress(stage, progress, message="", estimated_time=None):
    """ì§„í–‰ë¥  ì €ì¥"""
    os.makedirs(os.path.dirname(PROGRESS_FILE), exist_ok=True)
    progress_data = {
            'stage': stage,
            'progress': progress,
            'message': message,
            'timestamp': datetime.utcnow().isoformat()
        }
    if estimated_time is not None:
        progress_data['estimated_time_seconds'] = estimated_time
        progress_data['estimated_time_minutes'] = estimated_time / 60
    with open(PROGRESS_FILE, 'w') as f:
        json.dump(progress_data, f)

def get_influx_client():
    """InfluxDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return InfluxDBClient(url=INFLUXDB_URL, token=INFLUXDB_TOKEN, org=INFLUXDB_ORG)

def load_data_from_influxdb(client, days=7):
    """InfluxDBì—ì„œ ì¦ê°• ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“Š InfluxDBì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    print(f"ğŸ“¦ ë²„í‚·: {INFLUXDB_BUCKET_TEMP}, {INFLUXDB_BUCKET_VIB}")
    
    query_api = client.query_api()
    
    # ì˜¨ë„ ë°ì´í„° ì¡°íšŒ
    start_time = (datetime.utcnow() - timedelta(days=days)).strftime('%Y-%m-%dT%H:%M:%SZ')
    print(f"â° ì¡°íšŒ ê¸°ê°„: {start_time} ~ í˜„ì¬")
    
    temp_query = f'''
    from(bucket: "{INFLUXDB_BUCKET_TEMP}")
      |> range(start: {start_time})
      |> filter(fn: (r) => r["_measurement"] == "temperature")
      |> filter(fn: (r) => r["_field"] == "value")
      |> sort(columns: ["_time"])
    '''
    
    vib_query = f'''
    from(bucket: "{INFLUXDB_BUCKET_VIB}")
      |> range(start: {start_time})
      |> filter(fn: (r) => r["_measurement"] == "vibration")
      |> filter(fn: (r) => r["_field"] == "crest" or r["_field"] == "temperature")
      |> sort(columns: ["_time"])
    '''
    
    # ì˜¨ë„ ë°ì´í„° ìˆ˜ì§‘
    print(f"ğŸ” ì˜¨ë„ ë°ì´í„° ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... (ë²„í‚·: {INFLUXDB_BUCKET_TEMP})")
    try:
        temp_result = query_api.query(org=INFLUXDB_ORG, query=temp_query)
        temp_data = []
        table_count = 0
        record_count = 0
        
        for table in temp_result:
            table_count += 1
            for record in table.records:
                record_count += 1
                timestamp = record.get_time()
                value = record.get_value()
                if value is not None:
                    temp_data.append({
                        'time': timestamp,
                        'temperature': float(value)
                    })
        
        print(f"ğŸ“Š ì˜¨ë„ ì¿¼ë¦¬ ê²°ê³¼: í…Œì´ë¸” {table_count}ê°œ, ë ˆì½”ë“œ {record_count}ê°œ, ìœ íš¨ ë°ì´í„° {len(temp_data)}ê°œ")
    except Exception as e:
        print(f"âŒ ì˜¨ë„ ë°ì´í„° ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        temp_data = []
    
    # ì§„ë™ ë°ì´í„° ìˆ˜ì§‘
    print(f"ğŸ” ì§„ë™ ë°ì´í„° ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... (ë²„í‚·: {INFLUXDB_BUCKET_VIB})")
    try:
        vib_result = query_api.query(org=INFLUXDB_ORG, query=vib_query)
        vib_data = {}
        table_count = 0
        record_count = 0
        
        for table in vib_result:
            table_count += 1
            for record in table.records:
                record_count += 1
                timestamp = record.get_time()
                field = record.get_field()
                value = record.get_value()
                
                if value is not None:
                    if timestamp not in vib_data:
                        vib_data[timestamp] = {}
                    vib_data[timestamp][field] = float(value)
        
        print(f"ğŸ“Š ì§„ë™ ì¿¼ë¦¬ ê²°ê³¼: í…Œì´ë¸” {table_count}ê°œ, ë ˆì½”ë“œ {record_count}ê°œ, íƒ€ì„ìŠ¤íƒ¬í”„ {len(vib_data)}ê°œ")
    except Exception as e:
        print(f"âŒ ì§„ë™ ë°ì´í„° ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
        vib_data = {}
    
    print(f"âœ… ì˜¨ë„ ë°ì´í„°: {len(temp_data)}ê°œ, ì§„ë™ ë°ì´í„°: {len(vib_data)}ê°œ")
    
    # ë°ì´í„° ë³‘í•© (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ - pandas merge_asof ì‚¬ìš©)
    if not temp_data:
        error_msg = f"ì˜¨ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. '{INFLUXDB_BUCKET_TEMP}' ë²„í‚·ì— ì¦ê°• ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë°ì´í„° ì¦ê°•ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        print(f"âš ï¸ {error_msg}")
        raise ValueError(error_msg)
    
    # ì§„ë™ ë°ì´í„°ì—ì„œ crest í•„ë“œê°€ ìˆëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ
    vib_data_list = []
    for ts, fields in vib_data.items():
        if 'crest' in fields and fields['crest'] is not None:
            vib_data_list.append({
                'time': ts,
                'vibration_crest': fields['crest'],
                'vibration_temp': fields.get('temperature')
            })
    
    print(f"ğŸ“Š ì˜¨ë„ ë°ì´í„°: {len(temp_data)}ê°œ, ì§„ë™ ë°ì´í„°(crest í¬í•¨): {len(vib_data_list)}ê°œ")
    
    if not vib_data_list:
        error_msg = f"ì§„ë™ ë°ì´í„°ì— 'crest' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. '{INFLUXDB_BUCKET_VIB}' ë²„í‚·ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        print(f"âš ï¸ {error_msg}")
        raise ValueError(error_msg)
    
    # DataFrame ìƒì„±
    temp_df = pd.DataFrame(temp_data)
    vib_df = pd.DataFrame(vib_data_list)
    
    # time ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ datetimeì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    if not pd.api.types.is_datetime64_any_dtype(temp_df['time']):
        temp_df['time'] = pd.to_datetime(temp_df['time'])
    if not pd.api.types.is_datetime64_any_dtype(vib_df['time']):
        vib_df['time'] = pd.to_datetime(vib_df['time'])
    
    # timeì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
    temp_df = temp_df.set_index('time').sort_index()
    vib_df = vib_df.set_index('time').sort_index()
    
    # merge_asofë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë§¤ì¹­ (ìµœëŒ€ 1ë¶„ ì°¨ì´ í—ˆìš©)
    MAX_TIME_DIFF = pd.Timedelta(minutes=1)
    
    merged_df = pd.merge_asof(
        temp_df,
        vib_df,
        left_index=True,
        right_index=True,
        direction='nearest',
        tolerance=MAX_TIME_DIFF
    )
    
    # crestê°€ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
    merged_df = merged_df.dropna(subset=['temperature', 'vibration_crest'])
    
    # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
    merged_df = merged_df.reset_index()
    
    merged_data = merged_df.to_dict('records')
    
    print(f"âœ… ë§¤ì¹­ëœ ë°ì´í„°: {len(merged_data)}ê°œ (ì˜¨ë„ {len(temp_data)}ê°œ, ì§„ë™ {len(vib_data_list)}ê°œ ì¤‘)")
    
    if not merged_data:
        error_msg = f"ë³‘í•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ì‹¤íŒ¨ (ìµœëŒ€ {MAX_TIME_DIFF.total_seconds()}ì´ˆ ì°¨ì´ í—ˆìš©). ì˜¨ë„: {len(temp_data)}ê°œ, ì§„ë™(crest): {len(vib_data_list)}ê°œ"
        print(f"âš ï¸ {error_msg}")
        raise ValueError(error_msg)
    
    df = pd.DataFrame(merged_data)
    
    # 'time' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì •ë ¬
    if 'time' in df.columns and len(df) > 0:
        df = df.sort_values('time')
        df = df.reset_index(drop=True)
    else:
        print("âš ï¸ 'time' ì»¬ëŸ¼ì´ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['time', 'temperature', 'vibration_crest', 'vibration_temp'])
    
    print(f"âœ… ë³‘í•©ëœ ë°ì´í„°: {len(df)}ê°œ")
    return df

def create_sequences(data, seq_length):
    """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
    X, y_temp, y_vib = [], [], []
    
    for i in range(len(data) - seq_length):
        seq = data[i:i+seq_length]
        X.append(seq)
        
        # ë‹¤ìŒ ì‹œì ì˜ ì˜¨ë„ì™€ ì§„ë™ ì˜ˆì¸¡
        y_temp.append(data[i+seq_length, 0])  # temperature
        y_vib.append(data[i+seq_length, 1])   # vibration_crest
    
    return np.array(X), np.array(y_temp), np.array(y_vib)

class LSTMModel(nn.Module):
    """LSTM ëª¨ë¸ (PyTorch)"""
    def __init__(self, input_size, hidden_size=48, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.dropout = nn.Dropout(dropout)
        self.fc1 = nn.Linear(hidden_size, 24)  # 32 -> 24ë¡œ ê°ì†Œí•˜ì—¬ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
        self.fc2 = nn.Linear(24, 2)  # ì˜¨ë„ì™€ ì§„ë™ ë‘ ê°œ ì¶œë ¥
        self.relu = nn.ReLU()
        
    def forward(self, x):
        # LSTM forward
        lstm_out, _ = self.lstm(x)
        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶œë ¥ë§Œ ì‚¬ìš©
        last_output = lstm_out[:, -1, :]
        # Dropout
        out = self.dropout(last_output)
        # Fully connected layers
        out = self.relu(self.fc1(out))
        out = self.fc2(out)
        return out

def setup_device():
    """GPU/CPU ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    print("=" * 60)
    print("ğŸ” ë””ë°”ì´ìŠ¤ ì„¤ì • í™•ì¸ ì¤‘...")
    print("=" * 60)
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    
    # GPU ì‚¬ìš© ë¹„í™œì„±í™” (í¬ë˜ì‹œ ë°©ì§€)
    # GPUê°€ ê°ì§€ë˜ë”ë¼ë„ CPUë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
    print("âš ï¸ GPU ì‚¬ìš©ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # GPU ê°ì§€ ì‹œë„ (ì •ë³´ë§Œ í™•ì¸, ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    cuda_available = False
    try:
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"ğŸ’¡ GPUê°€ ê°ì§€ë˜ì—ˆì§€ë§Œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (CPU ëª¨ë“œ)")
    except Exception as e:
        print(f"ğŸ’¡ GPU ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œ): {e}")
        cuda_available = False
    
    # GPU ì‚¬ìš© ì‹œë„í•˜ì§€ ì•ŠìŒ (CPUë§Œ ì‚¬ìš©)
    if False:  # GPU ì‚¬ìš© ë¹„í™œì„±í™”
        try:
            device = torch.device('cuda')
            device_count = torch.cuda.device_count()
            print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {device_count}ê°œ")
            
            # ê° GPU ì •ë³´ ì¶œë ¥
            for i in range(device_count):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            
            # ROCm ì •ë³´
            if hasattr(torch.version, 'hip') and torch.version.hip:
                print(f"   ROCm ë²„ì „: {torch.version.hip}")
            else:
                print(f"   CUDA ë²„ì „: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'N/A'}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            if device_count > 0:
                for i in range(device_count):
                    memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    print(f"   GPU {i} ë©”ëª¨ë¦¬: {memory_total:.2f} GB")
            
            print("=" * 60)
            print("ğŸš€ GPU ëª¨ë“œë¡œ í•™ìŠµí•©ë‹ˆë‹¤!")
            print("=" * 60)
            return device, True
        except Exception as e:
            print(f"âš ï¸ GPU ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ’¡ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            cuda_available = False
    
    # CPU ëª¨ë“œë¡œ ì „í™˜
    device = torch.device('cpu')
    print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("ğŸ’¡ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:")
    print("   - AMD GPU: ROCm ë“œë¼ì´ë²„ ì„¤ì¹˜ í•„ìš”")
    print("   - NVIDIA GPU: CUDA/cuDNN ì„¤ì¹˜ í•„ìš”")
    
    # CPU ìµœì í™” ì„¤ì •
    num_threads = os.cpu_count() or 16
    torch.set_num_threads(num_threads)
    # OpenMP ìŠ¤ë ˆë“œë„ ì„¤ì • (NumPy ë“±ì—ì„œ ì‚¬ìš©)
    os.environ['OMP_NUM_THREADS'] = str(num_threads)
    os.environ['MKL_NUM_THREADS'] = str(num_threads)
    print(f"âš™ï¸ CPU ìµœì í™”: {num_threads}ê°œ ìŠ¤ë ˆë“œ ì‚¬ìš©")
    print(f"   OMP_NUM_THREADS={num_threads}, MKL_NUM_THREADS={num_threads}")
    print("=" * 60)
    print("ğŸŒ CPU ëª¨ë“œë¡œ í•™ìŠµí•©ë‹ˆë‹¤")
    print("=" * 60)
    return device, False

def train():
    """ëª¨ë¸ í•™ìŠµ"""
    print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (PyTorch)")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device, use_gpu = setup_device()
    if use_gpu:
        print("ğŸš€ GPU ëª¨ë“œë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸŒ CPU ëª¨ë“œë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    save_progress('start', 0, 'ëª¨ë¸ í•™ìŠµ ì‹œì‘')
    
    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    client = get_influx_client()
    
    try:
        # ë°ì´í„° ë¡œë“œ (5ì¼ë¡œ ê°ì†Œí•˜ì—¬ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•)
        save_progress('loading', 5, 'ë°ì´í„° ë¡œë“œ ì¤‘...')
        df = load_data_from_influxdb(client, days=5)  # 7ì¼ -> 5ì¼ë¡œ ê°ì†Œ
        
        if df.empty:
            error_msg = "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ì¦ê°•ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."
            print(f"âŒ {error_msg}")
            save_progress('error', 0, error_msg)
            return
        
        if len(df) < SEQUENCE_LENGTH + 1:
            error_msg = f"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ {SEQUENCE_LENGTH + 1}ê°œ í•„ìš”, í˜„ì¬ {len(df)}ê°œ. ë” ë§ì€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë ¤ë©´ ë°ì´í„° ì¦ê°•ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ ì¡°íšŒ ê¸°ê°„ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”."
            print(f"âŒ {error_msg}")
            save_progress('error', 0, error_msg)
            return
        
        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['temperature', 'vibration_crest']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            error_msg = f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}"
            print(f"âŒ {error_msg}")
            save_progress('error', 0, error_msg)
            return
        
        # ë°ì´í„° ì •ê·œí™”
        scaler = MinMaxScaler()
        data_scaled = scaler.fit_transform(df[['temperature', 'vibration_crest']].values)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_temp, y_vib = create_sequences(data_scaled, SEQUENCE_LENGTH)
        y = np.column_stack([y_temp, y_vib])
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")
        
        # ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìƒ˜í”Œë§ (í…ì„œ ìƒì„± ì „ì— ìˆ˜í–‰)
        # ë°°ì¹˜ ìˆ˜ë¥¼ 500ê°œ ì´í•˜ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        # ë°°ì¹˜ í¬ê¸° ìµœì†Œ 1024 ê°€ì •: 500 * 1024 = 512,000
        max_samples = 400000  # ìµœëŒ€ 40ë§Œ ê°œ ìƒ˜í”Œ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •)
        
        if len(X_train) > max_samples:
            print(f"âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({len(X_train):,}ê°œ).")
            print(f"ğŸ’¡ ë°ì´í„° ìƒ˜í”Œë§ì„ ì ìš©í•˜ì—¬ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤...")
            print(f"ğŸ“‰ ë°ì´í„° ìƒ˜í”Œë§: {len(X_train):,}ê°œ â†’ {max_samples:,}ê°œë¡œ ê°ì†Œ")
            
            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            step = max(1, len(X_train) // max_samples)
            indices = np.arange(0, len(X_train), step)[:max_samples]
            X_train = X_train[indices]
            y_train = y_train[indices]
            
            # ê²€ì¦ ë°ì´í„°ë„ ë¹„ìœ¨ì— ë§ê²Œ ì¡°ì • (ìµœëŒ€ 10ë§Œ ê°œ)
            max_val_samples = min(100000, len(X_val))
            if len(X_val) > max_val_samples:
                step_val = max(1, len(X_val) // max_val_samples)
                indices_val = np.arange(0, len(X_val), step_val)[:max_val_samples]
                X_val = X_val[indices_val]
                y_val = y_val[indices_val]
            
            print(f"âœ… ìƒ˜í”Œë§ ì™„ë£Œ: í•™ìŠµ {len(X_train):,}ê°œ, ê²€ì¦ {len(X_val):,}ê°œ")
        else:
            print(f"âœ… ë°ì´í„° ì–‘ ì ì ˆ: í•™ìŠµ {len(X_train):,}ê°œ, ê²€ì¦ {len(X_val):,}ê°œ")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        save_progress('preparing', 8, 'í…ì„œ ë³€í™˜ ì¤‘...')
        
        # PyTorch í…ì„œë¡œ ë³€í™˜
        print("ğŸ”„ í…ì„œ ë³€í™˜ ì¤‘...")
        print(f"  ğŸ“Š í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}")
        print(f"  ğŸ“Š ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_val.shape}")
        print(f"  ğŸ’¾ ë””ë°”ì´ìŠ¤: {device}")
        
        print("  ğŸ”„ í•™ìŠµ ë°ì´í„° í…ì„œ ë³€í™˜ ì¤‘...")
        X_train_tensor = torch.FloatTensor(X_train).to(device)
        print("  âœ… X_train í…ì„œ ë³€í™˜ ì™„ë£Œ")
        y_train_tensor = torch.FloatTensor(y_train).to(device)
        print("  âœ… y_train í…ì„œ ë³€í™˜ ì™„ë£Œ")
        
        print("  ğŸ”„ ê²€ì¦ ë°ì´í„° í…ì„œ ë³€í™˜ ì¤‘...")
        X_val_tensor = torch.FloatTensor(X_val).to(device)
        print("  âœ… X_val í…ì„œ ë³€í™˜ ì™„ë£Œ")
        y_val_tensor = torch.FloatTensor(y_val).to(device)
        print("  âœ… y_val í…ì„œ ë³€í™˜ ì™„ë£Œ")
        
        print("âœ… í…ì„œ ë³€í™˜ ì™„ë£Œ")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        save_progress('preparing', 9, 'DataLoader ìƒì„± ì¤‘...')
        
        # DataLoader ìƒì„±
        print("ğŸ”„ DataLoader ìƒì„± ì¤‘...")
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        print("âœ… DataLoader ìƒì„± ì™„ë£Œ")
        
        # ë°ì´í„° í¬ê¸° í™•ì¸
        total_samples = len(X_train)
        print(f"ğŸ“Š ì´ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ")
        
        # ë°°ì¹˜ í¬ê¸° ì¡°ì • (ë¬´ì¡°ê±´ ìµœì†Œ 1024 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë°°ì¹˜ ìˆ˜ ê°ì†Œ)
        num_workers = 0  # CPU ëª¨ë“œì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™” (ì•ˆì •ì„±)
        
        # ë°°ì¹˜ ìˆ˜ë¥¼ 500ê°œ ì´í•˜ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
        target_max_batches = 500  # ëª©í‘œ ìµœëŒ€ ë°°ì¹˜ ìˆ˜
        optimal_batch_size = max(1024, (total_samples + target_max_batches - 1) // target_max_batches)
        
        # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° ì„¤ì • (ìµœì†Œ 1024)
        if use_gpu:
            actual_batch_size = max(optimal_batch_size, 2048)  # GPUëŠ” ë” í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            num_workers = 4  # GPU ëª¨ë“œì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©
            print(f"ğŸš€ GPU ì‚¬ìš© ì¤‘: ë°°ì¹˜ í¬ê¸° {actual_batch_size}, workers: {num_workers}")
        else:
            # CPU ëª¨ë“œ: ë©”ëª¨ë¦¬ì— ê´€ê³„ì—†ì´ ìµœì†Œ 1024 ì´ìƒìœ¼ë¡œ ì„¤ì •
            if psutil:
                available_memory_gb = psutil.virtual_memory().available / (1024**3)
                if available_memory_gb > 32:
                    actual_batch_size = max(optimal_batch_size, 2048)  # ìµœì†Œ 2048
                    print(f"âš¡ CPU ìµœì í™”: ë°°ì¹˜ í¬ê¸° {actual_batch_size} (ë©”ëª¨ë¦¬ ì¶©ë¶„, ë°°ì¹˜ ìˆ˜ ìµœì†Œí™”)")
                elif available_memory_gb > 16:
                    actual_batch_size = max(optimal_batch_size, 1024)  # ìµœì†Œ 1024
                    print(f"âš¡ CPU ìµœì í™”: ë°°ì¹˜ í¬ê¸° {actual_batch_size} (ë©”ëª¨ë¦¬ ë³´í†µ, ë°°ì¹˜ ìˆ˜ ìµœì†Œí™”)")
                else:
                    actual_batch_size = max(optimal_batch_size, 1024)  # ìµœì†Œ 1024 (ë©”ëª¨ë¦¬ ë¶€ì¡±í•´ë„)
                    print(f"âš¡ CPU ìµœì í™”: ë°°ì¹˜ í¬ê¸° {actual_batch_size} (ë©”ëª¨ë¦¬ ë¶€ì¡±, í•˜ì§€ë§Œ ë°°ì¹˜ ìˆ˜ ìµœì†Œí™” ìš°ì„ )")
            else:
                # psutilì´ ì—†ì–´ë„ ìµœì†Œ 1024ë¡œ ì„¤ì •
                actual_batch_size = max(optimal_batch_size, 1024)
                print(f"âš¡ CPU ì‚¬ìš© ì¤‘: ë°°ì¹˜ í¬ê¸° {actual_batch_size} (psutil ì—†ìŒ, ë°°ì¹˜ ìˆ˜ ìµœì†Œí™”)")
        
        # ì˜ˆìƒ ë°°ì¹˜ ìˆ˜ ê³„ì‚° ë° ì¶œë ¥
        estimated_batches = (total_samples + actual_batch_size - 1) // actual_batch_size
        print(f"ğŸ“¦ ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches:,}ê°œ (ë°°ì¹˜ í¬ê¸°: {actual_batch_size})")
        
        # ë°°ì¹˜ ìˆ˜ê°€ ì—¬ì „íˆ ë§ìœ¼ë©´ ê°•ì œë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
        if estimated_batches > 500:
            print(f"âš ï¸ ê²½ê³ : ë°°ì¹˜ ìˆ˜ê°€ ì—¬ì „íˆ ë§ìŠµë‹ˆë‹¤ ({estimated_batches:,}ê°œ).")
            print(f"ğŸ’¡ ë°°ì¹˜ í¬ê¸°ë¥¼ ê°•ì œë¡œ ì¦ê°€ì‹œí‚µë‹ˆë‹¤...")
            # ë°°ì¹˜ ìˆ˜ê°€ 500ê°œê°€ ë˜ë„ë¡ ë°°ì¹˜ í¬ê¸° ì¬ê³„ì‚°
            actual_batch_size = (total_samples + 499) // 500  # ì˜¬ë¦¼ ì²˜ë¦¬
            actual_batch_size = max(actual_batch_size, 1024)  # ìµœì†Œ 1024 ìœ ì§€
            estimated_batches = (total_samples + actual_batch_size - 1) // actual_batch_size
            print(f"âœ… ì¡°ì •ëœ ë°°ì¹˜ í¬ê¸°: {actual_batch_size}, ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {estimated_batches:,}ê°œ")
        
        # DataLoader ìƒì„± (num_workers=0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì•ˆì •ì„± í™•ë³´)
        print(f"ğŸ“¦ DataLoader ì„¤ì •: batch_size={actual_batch_size}, num_workers={num_workers}")
        train_loader = DataLoader(train_dataset, batch_size=actual_batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)
        val_loader = DataLoader(val_dataset, batch_size=actual_batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)
        
        # ëª¨ë¸ êµ¬ì¶• (ëª¨ë¸ í¬ê¸° ê°ì†Œë¡œ í•™ìŠµ ì‹œê°„ ë‹¨ì¶•)
        print("ğŸ—ï¸ ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        model = LSTMModel(input_size=2, hidden_size=48, num_layers=2, dropout=0.2).to(device)  # hidden_size 64 -> 48ë¡œ ê°ì†Œ
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ")
        
        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        
        # í•™ìŠµ ì‹œì‘
        print("ğŸš€ í•™ìŠµ ë£¨í”„ ì‹œì‘...")
        save_progress('training', 10, f'ëª¨ë¸ í•™ìŠµ ì‹œì‘... (ì—í¬í¬ {EPOCHS}ê°œ, ë°°ì¹˜ í¬ê¸° {actual_batch_size})')
        best_val_loss = float('inf')
        patience = 5  # Early stopping patience ê°ì†Œ (10 -> 5)ë¡œ ë¹ ë¥¸ ì¢…ë£Œ
        patience_counter = 0
        
        # ì „ì²´ í•™ìŠµ ì‹œì‘ ì‹œê°„ ì¶”ì 
        training_start_time = time.time()
        
        # í•™ìŠµ ë°ì´í„°ë¡œë” ì •ë³´ ì¶œë ¥
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°ë¡œë”: {len(train_loader)}ê°œ ë°°ì¹˜")
        print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°ë¡œë”: {len(val_loader)}ê°œ ë°°ì¹˜")
        
        # ì „ì²´ ë°°ì¹˜ ìˆ˜ ê³„ì‚° (ì§„í–‰ë¥  ê³„ì‚°ìš©)
        total_epochs = EPOCHS
        batches_per_epoch = len(train_loader)
        total_all_batches = total_epochs * batches_per_epoch
        
        for epoch in range(EPOCHS):
            # í•™ìŠµ ëª¨ë“œ
            model.train()
            train_loss = 0.0
            
            print(f"ğŸ”„ ì—í¬í¬ {epoch + 1}/{EPOCHS} ì‹œì‘")
            print(f"  ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘... (ì´ {batches_per_epoch}ê°œ ë°°ì¹˜)")
            batch_count = 0
            total_batches = batches_per_epoch
            epoch_start_time = time.time()
            
            for batch_X, batch_y in train_loader:
                batch_start_time = time.time()
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
                batch_count += 1
                
                batch_time = time.time() - batch_start_time
                
                # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚°: (ì™„ë£Œëœ ë°°ì¹˜ ìˆ˜ / ì „ì²´ ë°°ì¹˜ ìˆ˜) * 90 + 10
                completed_batches = (epoch * batches_per_epoch) + batch_count
                total_progress_percent = int((completed_batches / total_all_batches) * 90) + 10
                # 100%ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ
                total_progress_percent = min(total_progress_percent, 100)
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ í›„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if batch_count == 1:
                    estimated_epoch_time = batch_time * total_batches  # í˜„ì¬ ì—í¬í¬ ì˜ˆìƒ ì‹œê°„
                    estimated_total_time = estimated_epoch_time * EPOCHS  # ì „ì²´ ì—í¬í¬ ì˜ˆìƒ ì‹œê°„
                    print(f"  âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ (Loss: {loss.item():.4f}, ì†Œìš” ì‹œê°„: {batch_time:.2f}ì´ˆ)")
                    print(f"  â±ï¸ í˜„ì¬ ì—í¬í¬ ì˜ˆìƒ ì‹œê°„: {estimated_epoch_time/60:.1f}ë¶„ ({estimated_epoch_time:.0f}ì´ˆ)")
                    print(f"  â±ï¸ ì „ì²´ í•™ìŠµ ì˜ˆìƒ ì‹œê°„: {estimated_total_time/60:.1f}ë¶„ ({estimated_total_time:.0f}ì´ˆ)")
                    save_progress('training', total_progress_percent, 
                                f'ì—í¬í¬ {epoch + 1}/{EPOCHS} í•™ìŠµ ì¤‘... (ë°°ì¹˜ {batch_count}/{total_batches})', 
                                estimated_time=estimated_total_time)
                
                # ë°°ì¹˜ ìˆ˜ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¡°ì •
                # ë°°ì¹˜ê°€ ë§ìœ¼ë©´ ë” ìì£¼ ì—…ë°ì´íŠ¸ (ë§¤ ë°°ì¹˜ë§ˆë‹¤ ë˜ëŠ” ë§¤ 2-3ê°œë§ˆë‹¤)
                update_interval = 1 if total_batches > 1000 else (3 if total_batches > 500 else 5)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë” ìì£¼)
                if batch_count % update_interval == 0:
                    # ì „ì²´ í•™ìŠµ ì§„í–‰ë¥  ê¸°ë°˜ìœ¼ë¡œ ë‚¨ì€ ì‹œê°„ ê³„ì‚°
                    total_elapsed_time = time.time() - training_start_time
                    # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚°: (ì™„ë£Œëœ ë°°ì¹˜ ìˆ˜ / ì „ì²´ ë°°ì¹˜ ìˆ˜)
                    total_progress = completed_batches / total_all_batches
                    
                    # ì§„í–‰ë¥ ì´ 0ë³´ë‹¤ í¬ë©´ ë‚¨ì€ ì‹œê°„ ê³„ì‚°
                    if total_progress > 0:
                        estimated_total_time = total_elapsed_time / total_progress
                        estimated_remaining = estimated_total_time - total_elapsed_time
                    else:
                        estimated_remaining = 0
                    
                    message = f'ì—í¬í¬ {epoch + 1}/{EPOCHS} í•™ìŠµ ì¤‘... (ë°°ì¹˜ {batch_count}/{total_batches})'
                    if estimated_remaining > 0:
                        message += f' [ë‚¨ì€ ì‹œê°„: ì•½ {estimated_remaining/60:.1f}ë¶„]'
                    
                    save_progress('training', total_progress_percent, message)
                    
                    # ë¡œê·¸ ì¶œë ¥ (10ê°œ ë°°ì¹˜ë§ˆë‹¤ ë˜ëŠ” ë°°ì¹˜ê°€ ë§ìœ¼ë©´ 50ê°œë§ˆë‹¤)
                    log_interval = 50 if total_batches > 1000 else 10
                    if batch_count % log_interval == 0:
                        # í‰ê·  ë°°ì¹˜ ì‹œê°„ ê³„ì‚°
                        elapsed_time = time.time() - epoch_start_time
                        avg_batch_time = elapsed_time / batch_count if batch_count > 0 else 0
                        print(f"  ğŸ“Š {batch_count}/{total_batches} ë°°ì¹˜ ì™„ë£Œ (ì „ì²´ ì§„í–‰ë¥ : {total_progress_percent}%, Loss: {loss.item():.4f}, í‰ê·  ë°°ì¹˜ ì‹œê°„: {avg_batch_time:.2f}ì´ˆ)")
            
            print(f"  âœ… ì—í¬í¬ {epoch + 1} í•™ìŠµ ì™„ë£Œ (ì´ {batch_count}ê°œ ë°°ì¹˜ ì²˜ë¦¬)")
            
            # ê²€ì¦ ëª¨ë“œ
            print(f"  ğŸ” ê²€ì¦ ì‹œì‘... (ì´ {len(val_loader)}ê°œ ë°°ì¹˜)")
            model.eval()
            val_loss = 0.0
            val_batch_count = 0
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    val_batch_count += 1
                    # ì²« ë²ˆì§¸ ê²€ì¦ ë°°ì¹˜ ì™„ë£Œ ì‹œ ë¡œê·¸
                    if val_batch_count == 1:
                        print(f"  âœ… ì²« ë²ˆì§¸ ê²€ì¦ ë°°ì¹˜ ì™„ë£Œ")
            
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì—í¬í¬ ì™„ë£Œ ì‹œ)
            completed_batches = (epoch + 1) * batches_per_epoch
            progress = int((completed_batches / total_all_batches) * 90) + 10
            progress = min(progress, 100)
            save_progress('training', progress, f'ì—í¬í¬ {epoch + 1}/{EPOCHS} ì™„ë£Œ (Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f})')
            
            print(f"âœ… Epoch {epoch + 1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                best_model_path = os.path.join(MODEL_DIR, 'best_model.pth')
                torch.save(model.state_dict(), best_model_path)
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"â¹ï¸ Early stopping at epoch {epoch + 1}")
                    # ìµœê³  ëª¨ë¸ ë¡œë“œ
                    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'best_model.pth')))
                    break
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        save_progress('saving', 95, 'ëª¨ë¸ ì €ì¥ ì¤‘...')
        final_model_path = os.path.join(MODEL_DIR, 'model.pth')
        # ì‹¤ì œ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì €ì¥ (hidden_size=48)
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'input_size': 2,
                'hidden_size': 48,  # ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
                'num_layers': 2,
                'dropout': 0.2
            }
        }, final_model_path)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        scaler_path = os.path.join(MODEL_DIR, 'scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        
        save_progress('complete', 100, 'ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!')
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_path}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        save_progress('error', 0, f'ì˜¤ë¥˜ ë°œìƒ: {str(e)}')
        import traceback
        traceback.print_exc()
    finally:
        client.close()

if __name__ == '__main__':
    train()
